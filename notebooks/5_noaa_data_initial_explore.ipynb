{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NOAA RATPAC-B Data\n",
        "\n",
        "-----\n",
        "\n",
        "## Initial Data Exploration\n",
        "\nInitial data exploration for the NOAA RATPAC-B temperature data."
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import pickle\n",
        "import calendar\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import interpolate as interp\n",
        "\n",
        "# - Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
        "import cartopy.crs as ccrs"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "proc_data_dir = '../data/processed'\n",
        "\n",
        "with open(os.path.join(proc_data_dir, 'ratpac_stations.pkl'), 'rb') as fin:\n",
        "    station_data = pickle.load(fin)\n",
        "\n",
        "with open(os.path.join(proc_data_dir, 'ratpac_b.pkl'), 'rb') as fin:\n",
        "    ratpac_data = pickle.load(fin)\n",
        "\n",
        "# - Merge to add location data and dates\n",
        "all_data = ratpac_data.join(station_data.loc[:, ['LAT', 'LON']], on='station_number')\n",
        "\n",
        "dt_data = np.empty((all_data.shape[0], 1), dtype=datetime)\n",
        "for row_id, row_vals in enumerate(all_data.values):\n",
        "    dt_data[row_id] = datetime(row_vals[1], row_vals[2], \n",
        "                               calendar.monthrange(row_vals[1], row_vals[2])[1])\n",
        "all_data['Date'] = dt_data\n",
        "\nall_data.drop(['year', 'month', 'station_number', 'station_id'], axis=1, inplace=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# - Interested in the Surface and 200 through 70mb data from 1965 onward\n",
        "altitudes = ['200mb', '150mb', '100mb', '70mb']\n",
        "cut_data = all_data.loc[:, ['Date', 'LON', 'LAT', 'surface'] + altitudes].copy()\n",
        "cut_data = cut_data.loc[cut_data.loc[:, 'Date'] >= datetime(1965, 1, 1)]\n",
        "\n",
        "altitude_weights = np.log([float(x[:-2]) for x in altitudes]).reshape(1, len(altitudes))\n",
        "\n",
        "all_alt_weights = pd.DataFrame(np.repeat(altitude_weights, cut_data.shape[0], 0),\n",
        "                               index=cut_data.index, columns=altitudes)\n",
        "all_alt_weights = all_alt_weights.where(~cut_data.loc[:, altitudes].isnull())\n",
        "all_alt_weights = all_alt_weights.div(all_alt_weights.sum(axis=1), axis=0)\n",
        "\n",
        "cut_data['aloft'] = cut_data.loc[:, altitudes].multiply(all_alt_weights).sum(axis=1)\n",
        "cut_data.drop(altitudes, axis=1, inplace=True)\n",
        "\n",
        "# - Need to reshape the data to make it easier to work with\n",
        "positions = list(zip(cut_data.loc[:, 'LON'].astype(float), \n",
        "                     cut_data.loc[:, 'LAT'].astype(float)))\n",
        "cut_data['Position'] = positions\n",
        "cut_data.drop(['LON', 'LAT'], axis=1, inplace=True)\n",
        "cut_data.set_index(['Date', 'Position'], inplace=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# - Create 3D Matrices: (date, longitude, latitude)\n",
        "unq_dates = np.sort(np.unique(cut_data.index.get_level_values('Date')))\n",
        "\n",
        "grid_names = ['surface', 'aloft']\n",
        "blank_grid = np.full((len(unq_dates), 360, 180), np.nan)\n",
        "\n",
        "raw_grids = dict()\n",
        "orig_point_data = dict()\n",
        "interp_point_data = dict()\n",
        "for nm in grid_names:\n",
        "    raw_grids[nm] = blank_grid.copy()\n",
        "    orig_point_data[nm] = list()\n",
        "    interp_point_data[nm] = list()\n",
        "\n",
        "for i_dt in range(len(unq_dates)):\n",
        "    dt = unq_dates[i_dt]\n",
        "    t_sub_df = cut_data.loc[dt]\n",
        "    \n",
        "    r_lons, r_lats = zip(*t_sub_df.index.values)\n",
        "    r_lons = np.array(r_lons)\n",
        "    r_lats = np.array(r_lats)\n",
        "    \n",
        "    for nm in grid_names:\n",
        "        t_values = t_sub_df.loc[:, nm].values\n",
        "        \n",
        "        idx_lons = np.round(r_lons).astype(int) + 180\n",
        "        idx_lats = np.round(r_lats).astype(int) + 90\n",
        "        raw_grids[nm][i_dt, idx_lons, idx_lats] = t_values\n",
        "        \n",
        "        t_points = np.concatenate([r_lons.reshape(-1, 1), r_lats.reshape(-1, 1)], axis=1)\n",
        "        t_points = t_points[~np.isnan(t_values)]\n",
        "        t_values = t_values[~np.isnan(t_values)]\n",
        "        orig_point_data[nm].append((t_points, t_values))\n",
        "        \n",
        "        # - Tesselate for boundary wrap (probably a better way to do this)\n",
        "        t_nrmpts = t_points.copy()\n",
        "        t_nrmpts[:, 0] += 180\n",
        "        t_nrmpts[:, 1] += 90\n",
        "\n",
        "        quad_pts = list()\n",
        "        quad_vls = list()\n",
        "        for i_x in range(3):\n",
        "            for i_y in range(3):\n",
        "                t_qpts = t_nrmpts.copy()\n",
        "                t_qpts[:, 0] += 360 * i_x\n",
        "                t_qpts[:, 1] += 180 * i_y\n",
        "\n",
        "                quad_pts.append(t_qpts)\n",
        "                quad_vls.append(t_values.copy())\n",
        "\n",
        "        quad_pts = np.concatenate(quad_pts, axis=0)\n",
        "        quad_vls = np.concatenate(quad_vls, axis=0)\n",
        "\n",
        "        quad_pts[:, 0] -= (360 + 180)\n",
        "        quad_pts[:, 1] -= (180 + 90)\n",
        "\n",
        "        qpt_mask = (quad_pts[:, 0] >= -360) & (quad_pts[:, 0] <= 360) \\\n",
        "                   & (quad_pts[:, 1] >= -180) & (quad_pts[:, 1] <= 180)\n",
        "\n",
        "        quad_pts = quad_pts[qpt_mask]\n",
        "        quad_vls = quad_vls[qpt_mask]\n",
        "        interp_point_data[nm].append((quad_pts, quad_vls))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# - Interpolate the data (radial basis function to smooth)\n",
        "method = 'gaussian'\n",
        "\n",
        "interp_results = dict()\n",
        "for nm in ['surface', 'aloft']:\n",
        "    print('Interpolating {} data... '.format(nm), end='', flush=True)\n",
        "    interp_results[nm] = np.zeros((len(unq_dates), 360, 180))\n",
        "    \n",
        "    for i_dt in range(len(unq_dates)):\n",
        "        intrp_pts, intrp_vals = interp_point_data[nm][i_dt]\n",
        "\n",
        "        gx, gy = np.mgrid[-360:360, -180:180]\n",
        "        rbfi = interp.Rbf(intrp_pts[:, 0], intrp_pts[:, 1], intrp_vals, function=method)\n",
        "        t_result = rbfi(gx, gy)\n",
        "\n",
        "        interp_results[nm][i_dt] = t_result[180:(180+360), 90:(90+180)]\n",
        "    \n",
        "    print('DONE')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# - Generate Plots\n",
        "i_dt = 0\n",
        "\n",
        "t_result = interp_results[i_dt]\n",
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
        "\n",
        "hmap = ax.imshow(t_result.T, extent=[-180, 180, -90, 90], interpolation='none', \n",
        "                 origin='lower', alpha=0.5, zorder=2)\n",
        "ax.scatter(orig_pts[:, 0], orig_pts[:, 1], marker='x', color='b', \n",
        "           zorder=3, alpha=1, label='Station')\n",
        "ax.coastlines(zorder=0, alpha=1.0)\n",
        "\n",
        "ax.axhline(0, linestyle='-', linewidth=0.75, color='grey', zorder=1)\n",
        "ax.axvline(0, linestyle='-', linewidth=0.75, color='grey', zorder=1)\n",
        "\n",
        "ax.set_title('Surface Average Temperature ({})'.format(pd.to_datetime(unq_dates[i_dt]) \\\n",
        "                                                       .strftime(\"%B %Y\")))\n",
        "ax.set_xlabel('Longitude')\n",
        "ax.xaxis.set_visible(True)\n",
        "\n",
        "maj_formatter = \n",
        "ax.set_ylabel('Latitude')\n",
        "ax.yaxis.set_visible(True)\n",
        "\n",
        "ax.grid(True, color='grey', linestyle='--', alpha=1, zorder=1)\n",
        "\nplt.show();"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "hurricanes-env"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "hurricanes-env",
      "language": "python",
      "display_name": "Hurricane Data Env"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}